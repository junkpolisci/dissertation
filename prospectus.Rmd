---
title: "Prospectus Outline"
author: "Christopher Junk"
date: Sys.Date()
output: 
  pdf_document:
    citation_package: natbib
    template: diss-chap1-latex-ms2.tex 
header-includes:
  - \usepackage{setspace}\doublespacing
bibliography: \dummy{`r file.path(getwd(), list.files(getwd(), "bib$"))`}
---

Do modern political science theories effectively explain the phenomena that are being studied? How can political scientists assess the validity of their theories? I believe these two questions capture the driving force of today's research agendas. Each researcher wants to understand the phenomena they have dedicated themselves to, and as a result they also want to provide the best evidence that their theory and hypotheses are correct and predict best [@muchlinski2016].

Null-hypothesis statistical testing (NHST) is the standard quantitative analytic approach in modern political science.  Since the advent of personal computers, and the subsequent availability of empirical modeling software such as Stata, R, Python, etc. it has become increasingly easy to estimate both simple and complex models to assess the validity of theories. This general process adheres to the following steps: 

1. Theorize and hypothesize about some phenomena.  
1. Establish some a priori heuristic to judge empirical analysis of the theory. Most commonly this is setting an alpha level of .05.  
1. Identify relevant data and collect it.  
1. Estimate models to assess the relevant null hypotheses ($H_0$) identified in step one.  
1. Using the heuristic established in step two infer what the models say about the validity of the theory and hypothesis of step one. Most commonly, if the p-value is less than or equal to the alpha level then the corresponding coefficient is considered statistically significant. 

This dissertation will focus on proposing a new methodological framework for quantitative analysis in political science that focuses on evaluating how well theories explain political events. The NHST approach over past decades offers only weak tools for analyzing how effectively variables of interest explain the outcome, and inference based on coefficients related to hypotheses is the central focus. Instead, I propose a paradigmatic shift in what political science should consider paramount. Specifically, the ultimate litmus test for whether a theory or hypothesis is good or not should be whether or not it improves out-of-sample prediction. Any variable that does not improve out-of-sample prediction is very likely not theoretically relevant to the event that occurs in its current operationalization. My framework, framework for out-of-sample prediction (FOOSP), argues that it is imperative to move theoretical evaluation toward prediction, and away from NHST. 

NHST depends on p-values and statistical significance to determine the validity of theories and hypotheses. The p-value is the proportion of the distribution of the test statistic (typically t-test or z statistic) that remains in the tails beyond the observed test statistc. For an explicit definition see @gailmard2014[p.244]: "The p-value is the probability, assuming $H_0$ is true, that the random test statistic is at least as extreme (relative to the expected value under $H_0$) as its observed value." Put into a mathematical expression a p-value is simply $P(Data|H_0)$ [@levine2008]. Thus, a p-value represents strictly the probability of observing the test statistic of a coefficient produced from the model estimated on a sample given $H_0$ is true. 

A p-value has very limited information to infer from when used correctly. At its best, it tells the researcher if the observed relationship is likely or unlikely given the null hypothesis. However, this interpretation is often stretched beyond this meaning and is interpreted in ways that are not accurate or scientific. First, it is commonplace for no strict alpha level to be set, and thus the criterion for statistical significance becomes ad hoc. Most authors use multiple levels of significance and stars to denote different p-values [e.g. @piazza2012]. This practice is so ingrained and common that it is the default of the 'esttab' package in Stata, which ouputs regression coefficient tables. Second, it is becoming common to include relatively large p-values as significant such as .1 [e.g. @daxecker2013; @berrebi2006] although it is questionable at this point whether one should feel safe rejecting the null hypothesis. Third, the interpretation of these different alpha levels as more or less evidence for a theory is also common. Take for example @bleek2014[(p. 442, emphasis added)]: "... security guarantees remained insignificant, although they came very close to the 10 percent threshold. For the reproduced logit model, security guarantees remained *extremely significantly* negatively correlated with all the three stages of proliferation behavior." In this example a small p-value is "extremely significant" in the authors' interpretation despite there being no such thing as an extremely significant p-value. In truth, they can only conclude with a high degree of certainty that it is unlikely to observe that relationship given that the null is true.

P-values offer very little in the way of scientific study of a phenomena. At best, they offer a probability that the null hypothesis is correct given data. This will be discussed in greater detail later, but I am not alone in my staunchness against p-values. Discussing his home field of psychology @meehl1979[p.817] wrote: "I believe that the almost universal reliance on merely refuting the null hypothesis as the standard method for corroborating substantive theories in the soft areas is a terrible mistake, basically unsound, poor scientific strategy, and one of the worst things that ever happened in the history of psychologgy." While his sentiments are stronger than mine, I agree in principal and will show that a fundamental analytic shift is needed. Statistical significance does not tell a researcher anything about how well a variable explains the outcome, and this is fundamentally the concern of scientific research. 

Notably, there are goodness of fit tests for each model type, and different way to analyze variance explained. In OLS there is $R^2$, the percent of variance explained by the model, and $\bar{R}^2$, the same as $R^2$ but penalized for the addition of variables to the model. Logistic regression has receiver operating characteristics (ROC) curves and the area under the curve (AUC). Each of these tools is useful in understanding how a model is fit to the sample used to train it, but none of them provide any information of how a model predicts or explains outside of the sample nor do they provide specific evidence to caution against overfitting. 

Also notable is that political science research almost always uses the full sample to train the model, and does not take into consideration overfitting the data. Overfitting occurs when a model "follows the errors, or noise, too closely [@james2017, p. 22]." When a model is fit on the entire sample, and nothing is withheld to test a model on, overfitting is near impossible to detect. Models that are overfit produce coefficients that are partially describing randomness, and thus are not generalizable. Inferences that are not generalizable are not useful outfside of the given sample. 

One further broad critique of the most common approach of political science quantitative research is the general lack of discussion of prediction. Given the central goal of explaining political phenomena, it is not possible to evaluate the explanatory power of a theory effectively without considering out-of-sample prediction. Any models not tested using out-of-sample prediction are prone to unidentified overfitting, and offer no evidence of a theories predictive power. A simple solution to this problem is to consider a model's predictive ability out-of-sample both with and without the variables of interest to the theory at hand. Then, the base model that excludes the new theoretical contribution can be used as a control for whether

The implementation of even simple machine learning approaches can resolve these problems. My framework is proposed as a set of increasingly more difficult tools to adopt, proposing that all research adopts the minimally invasive strategy in all contexts, and suggesting that all research strive to adopt as many strategies as possible. This framework also poses a fundamental shift from variable-by-variable significance testing to entire model (and thus theory) predictive assessment. This framework is based on the belief that a theory that cannot be used to effectively predict outside of the training sample is a bad theory, and thus should be discarded. 

# Problems with NHST  

The abuse of p-values as a metric for whether or not an idea or variable merits publication in a scientific setting is a sufficient problem in all quantiative fields that the American Statistical Assocaition recently released a statement on the use of p-values [@wasserstein2016a]. In this there are six principles, which are important to denote at the outset of the discussion of why NHST is problematic (pp. 131-132):  

1. "P-values can indicate how incompatible data are with a specified statistical model."   
1. "P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone."  
1. "Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specified threshold."  
1. "Proper inference requires full reporting and transparency."  
1. "A p-value, or statistical significance, does not measure the size of an effect or the importance of a result."  
1. "By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis."  

The first point suggests that p-values can indicate whether or not data is compatible with the null hypothesis. Small p-values indicate incompatability. Point two is elaborated to explain that the p-value contains information only about the relationship between the data and the specified null and alternative hypotheses, and that is it. Point three argues that binary decision-points are arbitrary, and adjudicating a coefficient's importance based on a p-value is misguided and incomplete. Point four encourages the sharing of all specifications, which guards against the practice of 'p-hacking', when a researcher estimates multiple models and only reports the most favorable ones. Points five and six are self-explanatory. 

The ASA statement affirms my argument above that p-values are a measurement regarding the probability distribution of a test statistic relevant to a null hypothesis. The use of a p-value outside of that setting is irresponsible and misleading. Nevertheless, the publication of insignificant, or 'null,' results is rare. This practice suggests that a significant coefficient is indicative of something important, and also indicates a belief as a field that significant results illuminate explanatory or predictive mechanisms. P-values provide no such mechanism, as will be discussed in detail below.

@breiman2001a further illuminates general problems with the NHST approach: what he calls data modeling (as opposed to algorithmic modeling) focuses on fitting models to full samples, and assessing their inferential value based on goodness of fit and p-values. As will be discussed further this approach is flawed because goodness of fit metrics are poor determinants of model fit in high-dimensional models [@breiman2001a; @landwehr1984; @bickel2006], and goodness of fits do not serve as good adjudicators or which model is 'correct' in a set of similarly well-fit models. 

Together, the faults of p-values and the limited nature of goodness-of-fit (GoF) analysis serve as two of the stronger arguments against NHST. Below, I enumerate many arguments against NHST, and expand on these. 

## Null and alternative hypotheses   

The null hypothesis in most political science research is that an explanatory variable has no relationship with a dependent variable: $H_0: \beta=0$ where $\beta$ represents the coefficient on a variable of interest from some model type. The alternative hypothesis in this context is almost always $H_A: \beta\neq0$. In almost every setting it is likely not true that any explanatory variable and a dependent variable have truly 0 relationship [@levine2008, p. 177]. Even if a relationship is small or mostly random it is not 0. Thus, the null hypothesis is never truly false, so the NHST framework is predicated on a strawman concept [@levine2008; @meehl1986]. 

Furthermore, it is important to further discuss the implications of a rejected null hypothesis. Given a sufficiently small p-value that one feels confident rejecting $H_0$ then the alternative hypothesis remains. In practice, the alternative hypothesis is purported to be of whatever formulation the author's theory leads to and is built on mechanisms sourced from a theory. 

Consider the timeless piece of @fearon2003, it is foundational in the civil war literature and has been cited 2118 times (according to the Cambridge University Press website, 6/28/2019). Their first hypothesis (p. 78) argues that more ethnically and religiously diverse countries will experience greater risk for civil war. Formally, $H_0: P(Civil\;War|Diversity) \le P(Civil\;War | \lnot Diversity)$ $H_A: P(Civil\;War|Diversity) > P(Civil\;War| \lnot Diversity)$. They use Stata and their replication materials indicate they run a simple logistic regression and estimate two-tailed z-tests to get p-values because that is the defualt in the Stata logistic command. Thus, despite their hypotheses being unidirectional, and one-tailed, the real stated null is $H_0: P(Civil\;War|Diversity) = P(Civil\;War | \lnot Diversity)$ and the alternative is $H_A: P(Civil\;War|Diversity) \neq P(Civil\;War| \lnot Diversity)$. The significance of the coefficients offers only sufficient information to reject the proposed relationship given the null of no difference between diverse and not diverse settings, and does not give sufficient information to confirm the proposed alternative hypothesis. Inasmuch as one has confidence in the model's specification and lack of biased coefficients, one can extend interpretation *independently of the p-values* to the coefficients, while still only concluding in terms of statistical significance that the relationship between civil war and diversity is not zero. 

Moreover, note that none of the empirically validated discussion of these models' findings relate to the theoretical mechanisms proposed in the paper. They argue that ethnic diversity creates tension between cultures because of governmental policies that create barriers between majority and minority groups. The more diversity one country experiences the more likely theses barriers spark conflict. In measurement, diversity is measured as the proportion of the population made up by the largest minority group. So, once more the null hypothesis is mispecified. In reality, the null is: $H_0: P(Civil\;War|Large\;Minority) = P(Civil\;War | \lnot Large\;Minority)$. And the alternative is: $H_A: P(Civil\;War|Large\;Minority) \neq P(Civil\;War | \lnot Large\;Minority)$.

Thus, a further abstraction is drawn between what is theorized and hypothesized and what is actually tested. It is difficult to impossible to perfectly measure theoretical concepts and hypothetical mechanisms, but it is important nonetheless to assure as much as possible that analyses reflect the hypotheses, and vice versa. Despite the fact that their results do not support this hypothesis, their analysis still shows signs of their readiness to misinterpret coefficients based on p-values. In discussion of the per capita income coefficients they say "... is strongly significant in botha  statistical and substantive sense ..."(p. 83). The notion of 'strong statistical signifiance' is completely misguided. A small p-value on the per capita income variable denotes that they are relatively confident in their ability to reject the null hypothesis that per capita income has no relationship with civil war onset, it does support the claim that per capita income definitely has a relationship with civil war onset. 

In sum, the NHST setup severely limits the intrinsic connection between what the null hypothesis being tested is and what the authors propose. In the seminal work of Fearon and Laitin there are two manifestations. First, they hypothesize about ethnic diversity, but only measure size of the largest minority. Thus, whether the empirical null hypothesis given that operationalization and their written implied null hypothesis is up for debate. Second, the interpretation of coefficients as strongly significant and rejecting the null hypothesis as validating the alternative hypothesis is incorrect. In a following section I discuss how $P(Data|H_0) \neq P(H_0|Data)$. 

## The impossibility of meeting a model's assumptions  

Also, this process is heavily dependent on the residuals of a model, which are dependent on meeting a model's assumptions. Consider simple linear regression (OLS) assumptions: (1) a model is linear and correctly specified, (2) there is not too much multicollinearity, (3) the disturbance term has zero expectation, (4) the disturbance term is homoscedastic, (5) the values of the disturbance term have independent distributions, (6) the disturbance term has a normal distribution [@dougherty2011a, pp. 159-160]. If each of these assumptions is not met then either the coefficients or the standard errors will be biased or wrong, and inference based on that model is wrong. 

As a simple proof of concept for why NHST with OLS (and all of its variants) are poorly suited for the study of international relations consider a setting in which the unit of anlaysis is country-year and the dependent variable is trade outflow or some continuous economic indicator. 

First, it is practically impossible on multiple fronts to satisfy the first assumption because it implies that all relevant variables are in the model. The only way the true model can be specified, and thus ommitted variable bias is eliminated, is if there is a way to know the true model. If there is no randomness in the dependent variable at all then it is possible to have a model with perfect fit, but that leads to the second near impossibility: identifying and gathering sufficient data and accurately measuring all of the relevant variables. Even given a list of what the perfect model is, obtaining complete data that is perfectly measured in political science is impossible at the country-year unit of analysis level. Especailly with economic data, many countries are unwilling to share. 

Second, and more damning to NHST, it is not practical or possible to perfectly account for the interdependence of observations on at least three fronts: spatial, temporal, and relational. One can technically model temporal correlation, spatial correlation, and relational dynamics (via network models), but doing all three together is certainly not practical. Not only would the model be incredibly complex and difficult to interpret, but identifying that model correctly is very difficult. From a temporal standpoint, one must identify the lag structure of the dependent variable and the lag structure of the independent varaibles necessary to get white noise residuals. From a spatial standpoint, one must account for spatial correlation in the residuals according to a similar set of potential lag structures but in a spatial plane rather than temporal one. 

Most importantly, without correctly accounting for problems with the residuals that include spatial and temporal correlation the standard errors of the ceofficients are incorrect and the t or z-statistics used to calculate p-values are then incorrect. In this setting, even if evaluating hypotheses based on p-values was a great idea it is very difficult to be sure that the p-values are correct in settings of complex data structures. This is just one of the many reasons that a common critique of p-values is that they are fickle to model specification [@basuchoudhary2018; @colaresi2017]. Transition toward predictive models in machine learning and focusing on adding to predictive power of models avoids these problems because they are not reliant on strict assumptions because they do not have concern with NHST.

## Limitations what p-values imply  

- in large samples, it is likely that a model will show a variable as statistically significant even if the author's specified relationship is false [@levine2008; @meehl1986]. 

## Dangers of fitting models on full samples  

## Dangers of Goodness of Fit tests  
Goodness of fit tests only consider how well a model fits the data, not how well it predicts the outcome [@breiman2001a].

- @breiman2001a argues that there are two schools for analysis: data modeling and algorithmic modeling. Data modeling is political science on average: fitting models on full samples looking to draw inference from coefficients and assess the models based on goodness of fit without considering cross-validation or potential problems of overfitting. In this school, a model with a significant goodness of fit statistic is considered a good explainer. Alternatively, algorithmic modeling considers much more advanced and complicated to interpret models that predict outcomes, and thus represent nature's process, better. 

In conclusion, the faults listed above of NHST cast significant doubt on its ability to meet the two fundamental goals of political science research: explanation of events, and assessment of theories. Strictly speaking, NHST does provide measures of explanation of variance *within the fit model*, but provides no such tool to test explanation out-of-sample. An overfit model can explain artificially high levels of variance because it tries to fit itself to randomnesss in the DV, which hampers generalizability and leads to incorrect inferences based on coefficients [@hill2013; @colaresi2017]. Aside from the problem of model fit and predictive performance, NHST provides no direct way to asses whether or not a model's variables actually add value to predicting the outcome of interest [@ward2010a; @gelman2014]. Coefficients offer insight into the correlational relationship between independent and dependent variables, and give some indication of the effect size within sample. Within sample effect size does not translate to out-of-sample prediction, and a model that only fits its training data well while performing poorly out of sample is not effective at identifying good theoretical arguments. NHST provides no methodology for assessing predictive power of the model, and thus provide no mechanism for validating a theory's usefulness. 

Given the conclusion that NHST fails to meet the needs of political scientists, I propose that whenever possible a new machine-learning based framework for out-of-sample prediction (FOOSP) supplants NHST. This framework meets the needs of political scientists for assessing theoretical validity and is actually simpler in understanding and implimentation than NHST because of it's weaker focus on standard errors and p-values and stronger focus on predictive accuracy. Ultimately, theories that predict poorly, or add no value to existing theoretically derived models, are bad theories. FOOSP provides a framework to more effectively identify predictive versus nonpredictive theories allowing the field in general to move forward in a more Lakatosian direction, and away from degenerative theory testing under NHST. 

Next, I discuss examples of work already moving in this direction and utilizing machine learning of all kinds on political science and other social science problems. Then, I explain my framework for analysis moving forward. 


# Machine Learning in Political Science: examples of FOOSP in practice

## Data collection/coding
* Several datasets that are commonly used in political science today use webscraping and machine learning a la natural language processing, specifically topic modeling and other classification algorithms [@pham2018; @nardulli2015; @boschee2019]

* @greene2018a use a variety of supervised machine learning techniques to assess the @fariss2014 claim that as human rights standards have increased over time, the human-coded scales of political repression have become less reliable because the reports themselves have changed. They find that classifiers trained on older State Department Human Rights Reports are worse predictors of more modern classifications, indicating that there are indeed dynamic reporting processes of human rights violations that need to be accounted for. 

* @minhas2015 use each stemmed and word (after removing stopwords) as features in an SVM to classify regime type according to the text the the state department Country Reports on Human Rights Practices and Freedom House's Freedom in the World reports. They achieve very high recall and accuracy according to the @geddes2014 and @hadenius2007. 

* @saiya2019 argue that decision-trees are dramatically under-utilized in predicting violence. They argue using the START GTD terrorism data that religious and secular terrorism are predicted using different factors, and are thus fundamentally different processes to predict. They advocate the use of gain-ratio analysis and proxy tree analysis. 

* @al-zewairi2017 uses four supervised machine learning approaches (distributed random forests, deep learning, naive bayes, and gradient boosting) to identify Islamic extremists from the Profiles of Individual Radicalization in the US [@start2018]). 

* @burscher2015 use the Passive Aggresive learning algorithm to create a classifier that correctly labels the policy type of a given news article. This is an example of using machine learning to assist coding, which is one of the most common uses in political science. 

* @croicu2015 use an ensemble method (built of SVM and NB, excluding random forests because of computational expense) to predict which articles are useful for identifying news articles that are relevant to protests to dramatically decrease the human-coding time needed to use news sources.

## Evaluation of existing theory
* @muchlinski2016 shows that random forests out-predict other forms of common models used in political science to predict rare events (logit, rare-events logit, L1-logit). Given a primary goal of theorizing about an outcome is predicting an outcome, it would be best to use the most effectively predicting models. 

* @basuchoudhary2018 tests the predictive accuracy of what the NHST literature to date has found. Premised on the argument that parametric NHST suffers from four main problems: not interested in predictive accuracy, p-values are sensitive to model specification, cannot effectively compare variables' explanatory power, nonstandardized robustness checks, does not distinguish correlation from causation. 

## Out of Sample Prediction 
* @gleditsch2013 use logit models trained on data up to 1990 to develop a model to predict conflict outcome after 1990 using the Issue Correlates of War (ICOW) data. 

* @blair2014 uses survey based local violence data across 240+ towns in Liberia to predict local violence in three different years. This is a prime example of high spatial variation but low temporal variation. 

* @shutte2017 use poisson point process models to analyze the GED data [@sundberg2013] at local levels. They use dirichilet tesselation to avoid information loss from data aggregation resulting from ad hoc gridding of data. They use leave-one-out cross-validation, leaving out one country to use for out-of-sample prediction. Their model fits reasonably well, but they find that more parsimonious (3 variable) models predict best.

* @witmer2017 have perhaps the most ambitious example of out-of-sample forecasting attempts. They used the ACLED data in 1-degree grid cells and simulated future climate data to predict subnational violence all the way into 2065. Obviously, this sort of work is impossible to validate, but it is quite a good example of the type of work that political science should strive for. While there is a strong argument to be made that this paper's forecasting model is relatively weak (most variables are country-level besides weather), the principal behind advocating predictive modeling should be more central to political science. The theories and research of political science is useful only inasmuch as it can actually identify when and where events will happen. 

* @hill2013 use random forests to predict culpability of of terrorism in the Philippines (where previously multinomial logit and naive bayes were used). They use cross-validation via out-of-bag prediction to guard against overfitting of the model and find that Random forests predict as well or better than the others as long as there is no class imbalance problem.

* @ding2017 use a variety of geo-coded data to predict terrorist events around the world in 2015. They achieve about a 96% accuracy rate using .1x.1 (latitude x longitude) grids around the world. They used neural networks, support vector machines, and random forests. 

* @mueller2018 use Latent Dirichlet Allocation (LDA) models to asses topics in newspaper articles to predict the onset of civil war. They use LASSO machine learning techniques to predict the onset of civil conflict using the results of unsupervised machine learning. This is a very common sense finding from very complicated methods that made it into the APSR

* @brandt2014 suggest that instead of RMSE and MAE probabilistic predictive models should be evaluated not by point metrics such as RMSE and MAE, but but a variety of tools common in meteorology that are optimized to assess predictive accuracy or probabilistic models. Many of these methods (ranked probability score for ordered categories of a discrete random variable, continuous rank probability scores for continuous options, verification rank histograms, probability integral transform, etc.). Each of these evaluation strategies focuses on different ways of considering how the predicted probability of an event matches up with the true outcome, and generally ranking by which predictions are best.

# Framework for out-of-sample prediction 

1. Theorize and hypothesize about some phenomena.  
1. Identify some a priori base model to which predictive accuracy will be compared. 
1. Identify and collect relevant data. Split this data into a training set and a test set. 
1. Fit a model(s) best suited for this data to a training set first without the new variables identifiedin step one, then with. Where practical and applicable, apply cross-validation strategies within the training set. 
1. Use the resulting models from step four to predict the values of the test set. Evaluate the accuracy of the test set using appropriate predictive accuracy assessment tools (e.g. point metrics (root mean squared error, mean absolute error) or forecast density evaluation). 
1. Compare the base model's predictive accuracy with the new model's predictive accuracy. Based on the change in predictive accuracy, infer about the validity of the proposed theory. 

Step one in this process is the same as NHST: identify some explanation of a phenomena that is interesting to political science that can be evaluated empirically. Step two is where NHST and FOOSP diverge: under FOOSP it is critical that a base model is identified. Ideally, this base model is the best predicting model the literature has to offer to date. In practice, this model is very difficult to identify given the variance in methods, samples, and concept operationalization. Thus, evidence in this context should only be interpreted inasmuch as the sample is generalizable. Ideally, one can identify the broadest sample, and the most general model that the literature has some moderate level of agreement on. Over time, this framework will be able to parse down and remove variables that add no predictive value. Furthermore, methods such as penalized regression models (e.g. ridge regression, and LASSO) or random forests can be used to evaluate variable importance for prediction. Variables that are not important for prediction, should not be include in favor of parsimony and avoiding overfitting. 

Step three is also similar in most ways to NHST, with the caveat of splitting the sample. A good rule of thumb is to keep anywhere from 33% [@colaresi2017] to 10% of the data as a test set and use the rest to train the model. Within the training set, cross-validate where possible and practical. Cross-validation is the process in which a training set is split into k parts, and a model is fit on k-1 parts, while the remaining part of the training set is used to evaluate; this process iteratures within the training set until each remaining k has been used as an evaluation set [@colaresi2017; @stone1974; @breiman2001a; @james2017, p 176]. This process iteratively improves models by identifying flaws on each iteration, and generating a composite model after each iteraturion. This is generally referred to as k-fold cross-validation, and it is difficult to impliment in contexts where spatio-temporal dynamics are part of the modeling process because the way you split the data fundamentally affects the modeling process. 

Other methods of cross-validating data are bootstrapping and bagging. Bootstrapping samples with replacement from the existing training set and estimates a model on this new bootstrapped training set. Bagging is best for high variance data, and its process involves bootstrapping several samples from the training data and averaging the resultant models' predictions to account for high variation [@james2017, pp. 316-317]. This is a common approach in decision-trees. 

Step four is simple enough: just fit the two models: base and updated model. If there are multiple hypotheses to be tested, perhaps multiple updated models should be estimated, carefully considering when to add more than one variable at a time. 

Step five is where FOOSP shines as a staunch improvement over NHST. Simply take the cross-validated models and test their predictive accuracy on the left out test set. There are a variety of methods this can take, the simplest of which was described above: leaving aside about 1/3rd of the data for a test set. Other methods include leave-one-out cross-validation (LOOCV); during step three fit the model to the entire sample minus one, and test prediction on the left-out observation, repeat this process for all observations and average the mean-squared error across all iterations [@james2017]. This is obviously computationally intensive, but provides extensive cross-validation to learn and improve models from. In the cases of bootstrapping and bagging, it is normal to test on the training set, or 'out-of-bag.' 

Step six simply involves evaluating the information retreived from the previous five steps. In an ideal case one finds that adding the variables measured for the theory and hypothesis have substantively significantly improved the model's predictive accuracy. In this setting statistical significance is *never* part of the discussion because standard errors and residuals in the GoF sense are completely irrelevant to the central concerns of FOOSP.  Below, I elaborate on specific methods that can and should be used such as ridge regression, LASSO, k-nearest neighbors (KNN), Naive Bayes, Random Forests, and Support Vector Machines. Each of these models, as well as standard OLS and other generalized linear models (e.g. logistic regression) and much more also fit nicely into this general framework. Also below, I elaborate further on exactly why freedom from the tyranny of GoF, standard errors, and p-values makes sience easier. 

## The tyranny of residuals in NHST and FOOSP's liberation

Simply put, the foundation of NHST as discussed above is that models meet their strict assumptions. Under these met strict assumptions standard errors are correct and p-values can be calculated and null hypotheses can be rejected, but alternative hypotheses cannot be confirmed. NHST also does not allow the examination of predictive accuracy out-of-sample. NHST provides almost no value for scientific advancement of theories, despite science's best effort for decades to extract value. 

FOOSP, alternatively, does not have any concern, or even discussion of standard errors, or p-values, and thus does not have to meet those strict assumptions. Predictive models either predict well out of sample, or they do not. Theories, hypotheses, and variables either add predicted value, or they do not. Whereas under NHST not properly modeling temporal or spatial dynamics can result in a virtually uninterpretable model from an inferential perspective, predictive-based modeling is largely unaffected. If space and time are important, and unmodelled, then the model predicts poorly, but it still predicts. Moreover, the predictions that it makes, while predicting poorly, are still interpretable even if they model performs poorly. 

Thus, whereas under NHST researchers can fail to model important correlation in the residuals and fallaciously interpret the results when they contain little truth, FOOSP directly punishes for poorly modeling real dynamics by predicting poorly. In NHST, this factor is never even considered, and many journals and referees do not ask for spatio-temporal models on average. 

## Explanation of models 

### Penalized Regression 

### K-nearest neighbors 

### Naive Bayes

### Random Forests 

### Support Vector Machines

# NHST v. FOOSP in practice: an empirical example
---
title: "Prospectus Outline"
author: "Christopher Junk"
date: Sys.Date()
output: 
  pdf_document:
    citation_package: natbib
    template: diss-chap1-latex-ms2.tex 
header-includes:
  - \usepackage{setspace}\doublespacing
bibliography: \dummy{`r file.path(getwd(), list.files(getwd(), "bib$"))`}
---

# Introduction 

> "Anticipating the future is both a social obligation and intellectual challenge
that no scientific discipline can escape."
> `r tufte::quote_footer('@schneider2010, p. 1')`

> "No matter how I turn it over in my mind, the number one task of peace research always turns out to be that of prediction" 
> `r tufte::quote_footer('@singer1973')`

>" . . . explanation in the absence of prediction is not scientifically superior to predictive analysis, it isn’t scientific at all! It is, instead, ‘pre-scientific’."
>> `r tufte::quote_footer('@schrodt2014, p. 290')`

Do modern political science theories effectively explain the phenomena that are being studied? How can political scientists assess the validity of their theories? I believe these two questions capture the driving force of today's research agendas. Each researcher wants to understand the phenomena they have dedicated themselves to, and as a result they also want to provide the best evidence that their theory and hypotheses are correct and predict best [@muchlinski2016; @singer1973].

Null-hypothesis statistical testing (NHST) is the standard quantitative analytic approach in modern political science.  Since the advent of personal computers, and the subsequent availability of empirical modeling software such as Stata, R, Python, etc. it has become increasingly easy to estimate both simple and complex models to assess the validity of theories. This general process adheres to the following steps: 

1. Theorize and hypothesize about some phenomena.  
1. Establish some a priori heuristic to judge empirical analysis of the theory. Most commonly this is setting an alpha level of .05.  
1. Identify relevant data and collect it.  
1. Estimate models to assess the relevant null hypotheses ($H_0$) identified in step one.  
1. Using the heuristic established in step two infer what the models say about the validity of the theory and hypothesis of step one. Most commonly, if the p-value is less than or equal to the alpha level then the corresponding coefficient is considered statistically significant. 

This dissertation will focus on proposing a new methodological framework for quantitative analysis in political science that focuses on evaluating how well theories explain political events. The NHST approach over past decades offers only weak tools for analyzing how effectively variables of interest explain the outcome, and inference based on coefficients related to hypotheses is the central focus. Instead, I propose a paradigmatic shift in what political science should consider paramount. Specifically, the ultimate litmus test for whether a theory or hypothesis is good or not should be whether or not it improves out-of-sample prediction. Any variable that does not improve out-of-sample prediction is very likely not theoretically relevant to the event that occurs in its current operationalization. My framework, framework for out-of-sample prediction (FOOSP), argues that it is imperative to move theoretical evaluation toward prediction, and away from NHST. 

NHST depends on p-values and statistical significance to determine the validity of theories and hypotheses. The p-value is the proportion of the distribution of the test statistic (typically t-test or z statistic) that remains in the tails beyond the observed test statistc. For an explicit definition see @gailmard2014[p.244]: "The p-value is the probability, assuming $H_0$ is true, that the random test statistic is at least as extreme (relative to the expected value under $H_0$) as its observed value." Put into a mathematical expression a p-value is simply $P(Data|H_0)$ [@levine2008]. Thus, a p-value represents strictly the probability of observing the test statistic of a coefficient produced from the model estimated on a sample given $H_0$ is true. 

A p-value has very limited information to infer from when used correctly. At its best, it tells the researcher if the observed relationship is likely or unlikely given the null hypothesis. However, this interpretation is often stretched beyond this meaning and is interpreted in ways that are not accurate or scientific. First, it is commonplace for no strict alpha level to be set, and thus the criterion for statistical significance becomes ad hoc. Most authors use multiple levels of significance and stars to denote different p-values [e.g. @piazza2012]. This practice is so ingrained and common that it is the default of the 'esttab' package in Stata, which ouputs regression coefficient tables. Second, it is becoming common to include relatively large p-values as significant such as .1 [e.g. @daxecker2013; @berrebi2006; @young2013] although it is questionable at this point whether one should feel safe rejecting the null hypothesis. Third, the interpretation of these different alpha levels as more or less evidence for a theory is also common. Take for example @bleek2014[(p. 442, emphasis added)]: "... security guarantees remained insignificant, although they came very close to the 10 percent threshold. For the reproduced logit model, security guarantees remained *extremely significantly* negatively correlated with all the three stages of proliferation behavior." In this example a small p-value is "extremely significant" in the authors' interpretation despite there being no such thing as an extremely significant p-value. In truth, they can only conclude with a high degree of certainty that it is unlikely to observe that relationship given that the null is true.

P-values offer very little in the way of scientific study of a phenomena. At best, they offer a probability that the null hypothesis is correct given data. This will be discussed in greater detail later, but I am not alone in my staunchness against p-values. Discussing his home field of psychology @meehl1979[p.817] wrote: "I believe that the almost universal reliance on merely refuting the null hypothesis as the standard method for corroborating substantive theories in the soft areas is a terrible mistake, basically unsound, poor scientific strategy, and one of the worst things that ever happened in the history of psychologgy." While his sentiments are stronger than mine, I agree in principal and will show that a fundamental analytic shift is needed. Statistical significance does not tell a researcher anything about how well a variable explains the outcome, and this is fundamentally the concern of scientific research [@schrodt2014]. 

Notably, there are goodness of fit tests for each model type, and different ways to analyze variance explained. In OLS there is $R^2$, the percent of variance explained by the model, and $\bar{R}^2$, the same as $R^2$ but penalized for the addition of variables to the model. Logistic regression has receiver operating characteristics (ROC) curves and the area under the curve (AUC). Each of these tools is useful in understanding how a model is fit to the sample used to train it, but none of them provide any information of how a model predicts or explains outside of the sample nor do they provide specific evidence to caution against overfitting. Moreover, these residual-based methods for explanatory assessment are not very useful in a multi-dimensional space [@breiman2001a; @landwehr1984; @bickel2006]

Also notable is that political science research almost always uses the full sample to train the model, and does not take into consideration overfitting the data. Overfitting occurs when a model "follows the errors, or noise, too closely [@james2017, p. 22]." When a model is fit on the entire sample, and nothing is withheld to test a model on, overfitting is near impossible to detect. Models that are overfit produce coefficients that are partially describing randomness, and thus are not generalizable. Inferences that are not generalizable are not useful outfside of the given sample. 

One further broad critique of the most common approach of political science quantitative research is the general lack of discussion of prediction. Given the central goal of explaining political phenomena, it is not possible to evaluate the explanatory power of a theory effectively without considering out-of-sample prediction. Any models not tested using out-of-sample prediction are prone to unidentified overfitting, and offer no evidence of a theories predictive power. A simple solution to this problem is to consider a model's predictive ability out-of-sample both with and without the variables of interest to the theory at hand. Then, the base model that excludes the new theoretical contribution can be used as a control for whether

The implementation of even simple machine learning approaches can resolve these problems. My framework is proposed as a set of increasingly more difficult tools to adopt, proposing that all research adopts the minimally invasive strategy in all contexts, and suggesting that all research strive to adopt as many strategies as possible. This framework also poses a fundamental shift from variable-by-variable significance testing to entire model (and thus theory) predictive assessment. This framework is based on the belief that theories should fundamentally be judged by their ability to predict and forecast out-of-sample, and theories that cannot predict, or improve prediction, are not good theories [@singer1973; @shrodt2014; @hegre2019; @schneider2010; @hegre2017]. 

# Problems with NHST  

The abuse of p-values as a metric for whether or not an idea or variable merits publication in a scientific setting is a sufficient problem in all quantiative fields that the American Statistical Assocaition recently released a statement on the use of p-values [@wasserstein2016a]. In this there are six principles, which are important to denote at the outset of the discussion of why NHST is problematic (pp. 131-132):  

1. "P-values can indicate how incompatible data are with a specified statistical model."   
1. "P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone."  
1. "Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specified threshold."  
1. "Proper inference requires full reporting and transparency."  
1. "A p-value, or statistical significance, does not measure the size of an effect or the importance of a result."  
1. "By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis."  

The first point suggests that p-values can indicate whether or not data is compatible with the null hypothesis. Small p-values indicate incompatability. Point two is elaborated to explain that the p-value contains information only about the relationship between the data and the specified null and alternative hypotheses, and that is it. Point three argues that binary decision-points are arbitrary, and adjudicating a coefficient's importance based on a p-value is misguided and incomplete. Point four encourages the sharing of all specifications, which guards against the practice of 'p-hacking', when a researcher estimates multiple models and only reports the most favorable ones. Points five and six are self-explanatory. 

The ASA statement affirms my argument above that p-values are a measurement regarding the probability distribution of a test statistic relevant to a null hypothesis. The use of a p-value outside of that setting is irresponsible and misleading. Nevertheless, the publication of insignificant, or 'null,' results is rare. This practice suggests that a significant coefficient is indicative of something important, and also indicates a belief as a field that significant results illuminate explanatory or predictive mechanisms. P-values provide no such mechanism, as will be discussed in detail below.

@breiman2001a further illuminates general problems with the NHST approach: what he calls data modeling (as opposed to algorithmic modeling) focuses on fitting models to full samples, and assessing their inferential value based on goodness of fit and p-values. As will be discussed further this approach is flawed because goodness of fit metrics are poor determinants of model fit in high-dimensional models [@breiman2001a; @landwehr1984; @bickel2006], and goodness of fits do not serve as good adjudicators or which model is 'correct' in a set of similarly well-fit models. 

Together, the faults of p-values and the limited nature of goodness-of-fit (GoF) analysis serve as two of the stronger arguments against NHST. Below, I enumerate many arguments against NHST, and expand on these. 

## Null and alternative hypotheses   

The null hypothesis in most political science research is that an explanatory variable has no relationship with a dependent variable: $H_0: \beta=0$ where $\beta$ represents the coefficient on a variable of interest from some model type. The alternative hypothesis in this context is almost always $H_A: \beta\neq0$. In almost every setting it is likely not true that any explanatory variable and a dependent variable have truly 0 relationship [@levine2008, p. 177]. Even if a relationship is small or mostly random it is not 0. Thus, the null hypothesis is never truly false, so the NHST framework is predicated on a strawman concept [@levine2008; @meehl1986]. 

Furthermore, it is important to further discuss the implications of a rejected null hypothesis. Given a sufficiently small p-value that one feels confident rejecting $H_0$ then the alternative hypothesis remains. In practice, the alternative hypothesis is purported to be of whatever formulation the author's theory leads to and is built on mechanisms sourced from a theory. 

Consider the timeless piece of @fearon2003, it is foundational in the civil war literature and has been cited 2118 times (according to the Cambridge University Press website, 6/28/2019). Their first hypothesis (p. 78) argues that more ethnically and religiously diverse countries will experience greater risk for civil war. Formally, $H_0: P(Civil\;War|Diversity) \le P(Civil\;War | \lnot Diversity)$ $H_A: P(Civil\;War|Diversity) > P(Civil\;War| \lnot Diversity)$. They use Stata and their replication materials indicate they run a simple logistic regression and estimate two-tailed z-tests to get p-values because that is the defualt in the Stata logistic command. Thus, despite their hypotheses being unidirectional, and one-tailed, the real stated null is $H_0: P(Civil\;War|Diversity) = P(Civil\;War | \lnot Diversity)$ and the alternative is $H_A: P(Civil\;War|Diversity) \neq P(Civil\;War| \lnot Diversity)$. The significance of the coefficients offers only sufficient information to reject the proposed relationship given the null of no difference between diverse and not diverse settings, and does not give sufficient information to confirm the proposed alternative hypothesis. Inasmuch as one has confidence in the model's specification and lack of biased coefficients, one can extend interpretation *independently of the p-values* to the coefficients, while still only concluding in terms of statistical significance that the relationship between civil war and diversity is not zero. 

Moreover, note that none of the empirically validated discussion of these models' findings relate to the theoretical mechanisms proposed in the paper. They argue that ethnic diversity creates tension between cultures because of governmental policies that create barriers between majority and minority groups. The more diversity one country experiences the more likely theses barriers spark conflict. In measurement, diversity is measured as the proportion of the population made up by the largest minority group. So, once more the null hypothesis is mispecified. In reality, the null is: $H_0: P(Civil\;War|Large\;Minority) = P(Civil\;War | \lnot Large\;Minority)$. And the alternative is: $H_A: P(Civil\;War|Large\;Minority) \neq P(Civil\;War | \lnot Large\;Minority)$.

Thus, a further abstraction is drawn between what is theorized and hypothesized and what is actually tested. It is difficult to impossible to perfectly measure theoretical concepts and hypothetical mechanisms, but it is important nonetheless to assure as much as possible that analyses reflect the hypotheses, and vice versa. Despite the fact that their results do not support this hypothesis, their analysis still shows signs of their readiness to misinterpret coefficients based on p-values. In discussion of the per capita income coefficients they say "... is strongly significant in both a statistical and substantive sense ..."(p. 83). The notion of 'strong statistical signifiance' is completely misguided. A small p-value on the per capita income variable denotes that they are relatively confident in their ability to reject the null hypothesis that per capita income has no relationship with civil war onset, it does support the claim that per capita income definitely has a relationship with civil war onset. 

In sum, the NHST setup severely limits the intrinsic connection between what the null hypothesis being tested is and what the authors propose. In the seminal work of Fearon and Laitin there are two manifestations. First, they hypothesize about ethnic diversity, but only measure size of the largest minority. Thus, whether the empirical null hypothesis given that operationalization and their written implied null hypothesis is up for debate. Second, the interpretation of coefficients as strongly significant and rejecting the null hypothesis as validating the alternative hypothesis is incorrect. In a following section I discuss how $P(Data|H_0) \neq P(H_0|Data)$. 

## The impossibility of meeting a model's assumptions  

Also, this process is heavily dependent on the residuals of a model, which are dependent on meeting a model's assumptions. Consider simple linear regression (OLS) assumptions: (1) a model is linear and correctly specified, (2) there is not too much multicollinearity, (3) the disturbance term has zero expectation, (4) the disturbance term is homoscedastic, (5) the values of the disturbance term have independent distributions, (6) the disturbance term has a normal distribution [@dougherty2011a, pp. 159-160]. If each of these assumptions is not met then either the coefficients or the standard errors will be biased or wrong, and inference based on that model is wrong. 

As a simple proof of concept for why NHST with OLS (and all of its variants) are poorly suited for the study of international relations consider a setting in which the unit of anlaysis is country-year and the dependent variable is trade outflow or some continuous economic indicator. 

First, it is practically impossible on multiple fronts to satisfy the first assumption because it implies that all relevant variables are in the model. The only way the true model can be specified, and thus ommitted variable bias is eliminated, is if there is a way to know the true model. If there is no randomness in the dependent variable at all then it is possible to have a model with perfect fit, but that leads to the second near impossibility: identifying and gathering sufficient data and accurately measuring all of the relevant variables. Even given a list of what the perfect model is, obtaining complete data that is perfectly measured in political science is impossible at the country-year unit of analysis level. Especailly with economic data, many countries are unwilling to share. 

Second, and more damning to NHST, it is not practical or possible to perfectly account for the interdependence of observations on at least three fronts: spatial, temporal, and relational. One can technically model temporal correlation, spatial correlation, and relational dynamics (via network models), but doing all three together is certainly not practical. Not only would the model be incredibly complex and difficult to interpret, but identifying that model correctly is very difficult. From a temporal standpoint, one must identify the lag structure of the dependent variable and the lag structure of the independent varaibles necessary to get white noise residuals. From a spatial standpoint, one must account for spatial correlation in the residuals according to a similar set of potential lag structures but in a spatial plane rather than temporal one. 

Most importantly, without correctly accounting for problems with the residuals that include spatial and temporal correlation the standard errors of the ceofficients are incorrect and the t or z-statistics used to calculate p-values are then incorrect. In this setting, even if evaluating hypotheses based on p-values was a great idea it is very difficult to be sure that the p-values are correct in settings of complex data structures. This is just one of the many reasons that a common critique of p-values is that they are fickle to model specification [@basuchoudhary2018; @colaresi2017]. Transition toward predictive models in machine learning and focusing on adding to predictive power of models avoids these problems because they are not reliant on strict assumptions because they do not have concern with NHST.

## P-values are specific, and rigid 

As briefly discussed above, p-values are designed and structured to offer very specific and limited information. A p-value is the probability that the following conditional probability is true: $P(data|H_0)$ [@gill1999]. In words, this means that a p-value represents *exclusively* the probability of observing the observed data, or data more extreme, given that $H_0$ is true. In the context of a regression, 'data' is the sample observed and the relationships within that sample, so that conditional can be rewritten to be $P(\beta|H_0)$, where $\beta$ is the coefficient representing some relationship. Thus, the p-value represents the probability of observing a coefficient given that the null hypothesis (usually $H_0:\beta = 0$): $p-value = P(\hat{\beta}|\beta=0)$. 

This is not interesting information relative to the conditional $P(H_0|data)$ which would be easier to infer from. Given the discussion above, this can be re-written as $P(\beta=0|\hat{\beta})$. If the p-value provides this information then the conclusions and inferences from NHST would indeed be much more interesting. Given Bayes law, $P(A|B) = \frac{P(A)}{P(B)}P(B|A)$, it is clear that $P(H_0|data) = P(data|H_0) \iff P(data) = P(H_0)$. Written alternatively in the context of the null and alternative hypotheses: $P(\beta = 0|\hat{\beta}) = P(\hat{\beta}|\beta = 0) \iff P(\hat{\beta}) = P(\beta = 0)$. There is no way have have any information regarding the true probability of either independent probability $P(\beta = 0)$ or $P(\hat{\beta})$ and thus it is impossible to ever conclude that $P(H_0|data) = P(data|H_0)$. 

## P-values are dependent on sample sizes in ways that coefficients are not

Apart from understanding exactly which conditional probability a p-value represents, consider the equations that obtain the p-values. The student's t for a $\beta$ coefficient in a two variable OLS equation[@dougherty2011]: 
$t = \frac{\hat{\beta_1}-\beta_1^0}{\frac{S_{\hat\beta_1}}{\sqrt{n}}}$. 
The equation for $\hat{\beta_1}$ is: 
$\hat{\beta_1} = \frac{\sum{(X_{1i} - \bar{X_1})(Y_i - \bar{Y})}\sum{(X_{2i}-\bar{X_2})^2} -  \sum{(X_{2i} - \bar{X_2})(Y_i - \bar{Y})}\sum{(X_{1i}-\bar{X_1})^2}}{\sum{(X_{1i} - \bar{X_1})}^2\sum{(X_{2i} - \bar{X_2})}^2 - [\sum{(X_{1i} - \bar{X_1})}\sum{(X_{2i} - \bar{X_2})}]^2}$
Immediately obvious from these two equations that makeup the the student's t value is that the equation for $\hat{\beta}$ is not impacted by n, the number of observations, while the standard error is downwardly weighted by an increased n. So, all else equal, as $n \to \infty$ the standard error decreases. Thus, as samples in analyses that use p-values and t-statistics (and z-scores) grow, test statistics also grow and low p-values become more common and thus less meaningful because it is purely a function of large datasets [@gill1999; @colaresi2017; @basuchoudhary2018; @levine2008; @meehl1986].

If it is easy to show mathematically that p-values are not reliable indicators of the truth of the null hypothesis with large sample sizes, then there is ample evidence to consider them a moot point in big-data analyses. For further visual and asymptotic evidence, consider the plots below generated from simulated data. Over 10,000 iterations I generated the same equation ($y = 1 + 3(x1) + 2(x2) + \epsilon$ where $\epsilon \sim N(0, 1)$) increasing the sample size by one for each iteration (starting at n = 4). After generating this data I refit the linear OLS model $Y = \alpha + \beta_1(x1) + \beta_2(x2) + \epsilon$ and plotted the results from the estimates. As you can see in Figure 1, student's t increases as n increases, with deviations because of the variance in the randomly generated data. Figure 2 shows the estimated coefficients for each iteration. These figures together show that as n increases, as long as the relationship between the independent variables and the dependent variable remain the same and the general distribution of those variables remain the same, p-values will get progressively smaller because student's t gets progressively larger. 

```{r Test statistic asymptotics, cache=T, eval=T, echo=F}
library(tidyverse)
loop <- data.frame(term = as.character(), 
                   estimate = as.numeric(), 
                   std.error = as.numeric(), 
                   statistic = as.numeric(), 
                   p.value = as.numeric(), 
                   n = as.numeric())

for(i in 4:10004){
  test <- data.frame(x1 = rnorm(i, 10, 2), 
                        x2 = rnorm(i, 20, 5), 
                        e = rnorm(i, 0, 1)) %>% 
    mutate(y = 1 + 3 * x1 + 2 * x2 + e)

  lm <-   broom::tidy(lm(y~x1+x2,
                 data = test)) %>% 
    mutate(n = i)
  
  loop <- rbind(loop, lm)
}

loop %>% 
  filter(term %in% c("x1", "x2")) %>% 
  ggplot(aes(x = n, 
             y = statistic)) + 
  geom_line() + 
  facet_wrap(.~term) + 
  labs(title = "Figure 1: T-Statistics for simple OLS as sample size increases",
       y = "Student's t", 
       x = "Sample Size", 
       caption = "Generative equation: y = 1 + 3(x1) + 2(x2) + e. e is distributed with mean 0 and SD 1.") + 
  theme(plot.title = element_text(hjust = .5))

loop %>% 
  filter(term %in% c("x1", "x2")) %>% 
  ggplot(aes(x = n, 
             y = estimate)) + 
  geom_line() + 
  facet_wrap(.~term) + 
  labs(title = "Figure 2: Coefficients for simple OLS as sample size increases",
       y = "Coefficient", 
       x = "Sample Size", 
       caption = "Generative equation: y = 1 + 3(x1) + 2(x2) + e. e is distributed with mean 0 and SD 1.") + 
  theme(plot.title = element_text(hjust = .5))

```

## Dangers of Goodness of Fit tests  
Goodness of fit tests only consider how well a model fits the data, not how well it predicts the outcome [@breiman2001a]. Particularly important to this section is the danger of overfitting, and how goodness of fit tests may not detect this. @beck2000 argues that focusing on GoF instead of out-of-sample predictio results in focusing on fitting models to idiosyncrasies rather than uncovering truly important structural dynamics. 

GoF tests are also problematic in that in high-dimensional spaces they do not always provide reliable inferences [@breiman2001a; @landwehr1984]. Furthermore, multiple different variable combinations can produce the same GoF values, thus making it impossible to determine which model is the best given the dependent variable, making it impossible to discern between theories. 

@ward2010a makes a clear inferential distinction between assessing models by their GoF and their out-of-sample prediction ability: full-sample models cannot offer any validation for causal arguments becauase the models explanatory ability within sample is merely a description of the data [@beck2000]. My argument here is not that predictive modeling has any causal analysis component. Rather, if one purports to understand the relationship between a set of predictors, and an outcome of interest, then considering a description of the training data (i.e. the model fit on the full sample) is not sufficient because of the potential of undetected overfitting of the model. 

The only way to truly avoid over fitting and adjudicate between models is to test models out-of-sample [@beck2000; @colaresi2017; @breiman2001a; @ward2010a; @hindman2015]. Testings out-of-sample can easily identify when a model is overfit or underfit: in both cases it will predict poorly. An underfit model (which will also be identifiable by GoF metrics) will simply not have enough information to predict well out-of-sample. An overfit model follows random movement that is atheoretical in the dependent variable, and thus inferences drawn from those models are not useful or accurate. Worse yet, it is not possible to reliably detect overfit models without out-of-sample prediction. The best bet is to use $\bar{R^2}$, which penalizes for lack of parsimony. 

@fafchamps2017 offer one of the most restrictive models to make out-of-sample testing a required facet of the publication process. They suggest that researchers should submit their collected data sample before analysis and a third-party splits the sample, only returning part of the sample to be used as a a training set for the project until publication. Once the project is accepted for publication the same model is then used to fit on the withheld portion of the sample to evaluate the theory's performance. This is distinct from predictive modeling because it still suggests using NHST on the test set, but the intuition of training and testing on separate datasets is similar. 

In conclusion, the faults listed above of NHST cast significant doubt on its ability to meet the two fundamental goals of political science research: explanation of events, and assessment of theories. Strictly speaking, NHST does provide measures of explanation of variance *within the fit model*, but provides no such tool to test explanation out-of-sample. An overfit model can explain artificially high levels of variance because it tries to fit itself to randomnesss in the DV, which hampers generalizability and leads to incorrect inferences based on coefficients [@hill2013; @colaresi2017]. Aside from the problem of model fit and predictive performance, NHST provides no direct way to asses whether or not a model's variables actually add value to predicting the outcome of interest [@ward2010a; @gelman2014]. Coefficients offer insight into the correlational relationship between independent and dependent variables, and give some indication of the effect size within sample. Within sample effect size does not translate to out-of-sample prediction, and a model that only fits its training data well while performing poorly out of sample is not effective at identifying good theoretical arguments. NHST provides no methodology for assessing predictive power of the model, and thus provide no mechanism for validating a theory's usefulness. 

Given the conclusion that NHST fails to meet the needs of political scientists, I propose that whenever possible a new machine-learning based framework for out-of-sample prediction (FOOSP) supplants NHST. This framework meets the needs of political scientists for assessing theoretical validity and is actually simpler in understanding and implimentation than NHST because of it's weaker focus on standard errors and p-values and stronger focus on predictive accuracy. Ultimately, theories that predict poorly, or add no value to existing theoretically derived models, are bad theories. FOOSP provides a framework to more effectively identify predictive versus nonpredictive theories allowing the field in general to move forward in a more Lakatosian direction, and away from degenerative theory testing under NHST. 

Next, I discuss examples of work already moving in this direction and utilizing machine learning of all kinds on political science and other social science problems. Then, I explain my framework for analysis moving forward. 


# Machine Learning in Political Science: examples of FOOSP in practice

## Data collection/coding 



## Data collection/coding (sources)
* Several datasets that are commonly used in political science today use webscraping and machine learning a la natural language processing, specifically topic modeling and other classification algorithms [@pham2018; @nardulli2015; @boschee2019]

* @greene2018a use a variety of supervised machine learning techniques to assess the @fariss2014 claim that as human rights standards have increased over time, the human-coded scales of political repression have become less reliable because the reports themselves have changed. They find that classifiers trained on older State Department Human Rights Reports are worse predictors of more modern classifications, indicating that there are indeed dynamic reporting processes of human rights violations that need to be accounted for. 

* @minhas2015 use each stemmed and word (after removing stopwords) as features in an SVM to classify regime type according to the text the the state department Country Reports on Human Rights Practices and Freedom House's Freedom in the World reports. They achieve very high recall and accuracy according to the @geddes2014 and @hadenius2007. 

* @saiya2019 argue that decision-trees are dramatically under-utilized in predicting violence. They argue using the START GTD terrorism data that religious and secular terrorism are predicted using different factors, and are thus fundamentally different processes to predict. They advocate the use of gain-ratio analysis and proxy tree analysis. 

* @al-zewairi2017 uses four supervised machine learning approaches (distributed random forests, deep learning, naive bayes, and gradient boosting) to identify Islamic extremists from the Profiles of Individual Radicalization in the US [@start2018]). 

* @burscher2015 use the Passive Aggresive learning algorithm to create a classifier that correctly labels the policy type of a given news article. This is an example of using machine learning to assist coding, which is one of the most common uses in political science. 

* @croicu2015 use an ensemble method (built of SVM and NB, excluding random forests because of computational expense) to predict which articles are useful for identifying news articles that are relevant to protests to dramatically decrease the human-coding time needed to use news sources.

## Evaluation of existing theory

Two examples that are the best representations of the framework that I am proposing the field switches to are @blair2019 and @ward2010a. @blair2019 tries two alternative approaches to predicting civil war escalation: one which treats theory as obsolete and irrelvant, thus considering all possible predictors, another which develops a theory based predictive model and they pit the models' predictive ability against each other as a way to discern whether or not theoretical foundations improve prediction. They conclude that the theoretically informed model does improve prediction, indicating that the era of big data and machine learning does not signal the end of theorizing. 

@ward2010a argue as I have that p-values are not useful for evaluating theory and they consider two popular models of civil war onset, @fearon2003 and @collier2004, in order to discern which theory predicts civil war onset better. They find that despite both papers validating their theories via p-values and the NHST approach, @collier2004 proved to be a much better predictive model. 

* @muchlinski2016 shows that random forests out-predict other forms of common models used in political science to predict rare events (logit, rare-events logit, L1-logit). Given a primary goal of theorizing about an outcome is predicting an outcome, it would be best to use the most effectively predicting models. 

* @basuchoudhary2018 tests the predictive accuracy of what the NHST literature to date has found. Premised on the argument that parametric NHST suffers from four main problems: not interested in predictive accuracy, p-values are sensitive to model specification, cannot effectively compare variables' explanatory power, nonstandardized robustness checks, does not distinguish correlation from causation. 

## Out of Sample Prediction 
* @gleditsch2013 use logit models trained on data up to 1990 to develop a model to predict conflict outcome after 1990 using the Issue Correlates of War (ICOW) data. 

* @blair2014 uses survey based local violence data across 240+ towns in Liberia to predict local violence in three different years. This is a prime example of high spatial variation but low temporal variation. 

* @shutte2017 use poisson point process models to analyze the GED data [@sundberg2013] at local levels. They use dirichilet tesselation to avoid information loss from data aggregation resulting from ad hoc gridding of data. They use leave-one-out cross-validation, leaving out one country to use for out-of-sample prediction. Their model fits reasonably well, but they find that more parsimonious (3 variable) models predict best.

* @witmer2017 have perhaps the most ambitious example of out-of-sample forecasting attempts. They used the ACLED data in 1-degree grid cells and simulated future climate data to predict subnational violence all the way into 2065. Obviously, this sort of work is impossible to validate, but it is quite a good example of the type of work that political science should strive for. While there is a strong argument to be made that this paper's forecasting model is relatively weak (most variables are country-level besides weather), the principal behind advocating predictive modeling should be more central to political science. The theories and research of political science is useful only inasmuch as it can actually identify when and where events will happen. 

* @hill2013 use random forests to predict culpability of of terrorism in the Philippines (where previously multinomial logit and naive bayes were used). They use cross-validation via out-of-bag prediction to guard against overfitting of the model and find that Random forests predict as well or better than the others as long as there is no class imbalance problem.

* @ding2017 use a variety of geo-coded data to predict terrorist events around the world in 2015. They achieve about a 96% accuracy rate using .1x.1 (latitude x longitude) grids around the world. They used neural networks, support vector machines, and random forests. 

* @mueller2018 use Latent Dirichlet Allocation (LDA) models to asses topics in newspaper articles to predict the onset of civil war. They use LASSO machine learning techniques to predict the onset of civil conflict using the results of unsupervised machine learning. This is a very common sense finding from very complicated methods that made it into the APSR

* @brandt2014 suggest that instead of RMSE and MAE probabilistic predictive models should be evaluated not by point metrics such as RMSE and MAE, but but a variety of tools common in meteorology that are optimized to assess predictive accuracy or probabilistic models. Many of these methods (ranked probability score for ordered categories of a discrete random variable, continuous rank probability scores for continuous options, verification rank histograms, probability integral transform, etc.). Each of these evaluation strategies focuses on different ways of considering how the predicted probability of an event matches up with the true outcome, and generally ranking by which predictions are best.

# Framework for out-of-sample prediction 

1. Theorize and hypothesize about some phenomena.  
1. Identify some a priori base model to which predictive accuracy will be compared. 
1. Identify and collect relevant data. Split this data into a training set and a test set. 
1. Fit a model(s) best suited for this data to a training set first without the new variables identifiedin step one, then with. Where practical and applicable, apply cross-validation strategies within the training set. 
1. Use the resulting models from step four to predict the values of the test set. Evaluate the accuracy of the test set using appropriate predictive accuracy assessment tools (e.g. point metrics (root mean squared error, mean absolute error) or forecast density evaluation). 
1. Compare the base model's predictive accuracy with the new model's predictive accuracy. Based on the change in predictive accuracy, infer about the validity of the proposed theory. 

Step one in this process is the same as NHST: identify some explanation of a phenomena that is interesting to political science that can be evaluated empirically. Step two is where NHST and FOOSP diverge: under FOOSP it is critical that a base model is identified. Ideally, this base model is the best predicting model the literature has to offer to date. In practice, this model is very difficult to identify given the variance in methods, samples, and concept operationalization. Thus, evidence in this context should only be interpreted inasmuch as the sample is generalizable. Ideally, one can identify the broadest sample, and the most general model that the literature has some moderate level of agreement on. Over time, this framework will be able to parse down and remove variables that add no predictive value. Furthermore, methods such as penalized regression models (e.g. ridge regression, and LASSO) or random forests can be used to evaluate variable importance for prediction. Variables that are not important for prediction, should not be include in favor of parsimony and avoiding overfitting. 

Step three is also similar in most ways to NHST, with the caveat of splitting the sample. A good rule of thumb is to keep anywhere from 33% [@colaresi2017] to 10% of the data as a test set and use the rest to train the model. Within the training set, cross-validate where possible and practical. Cross-validation is the process in which a training set is split into k parts, and a model is fit on k-1 parts, while the remaining part of the training set is used to evaluate; this process iteratures within the training set until each remaining k has been used as an evaluation set [@colaresi2017; @stone1974; @breiman2001a; @james2017, p 176]. This process iteratively improves models by identifying flaws on each iteration, and generating a composite model after each iteration. This is generally referred to as k-fold cross-validation, and it is difficult to implement in contexts where spatio-temporal dynamics are part of the modeling process because the way you split the data fundamentally affects the modeling process. 

Other methods of cross-validating data are bootstrapping and bagging. Bootstrapping samples with replacement from the existing training set and estimates a model on this new bootstrapped training set. Bagging is best for high variance data, and its process involves bootstrapping several samples from the training data and averaging the resultant models' predictions to account for high variation [@james2017, pp. 316-317]. This is a common approach in decision-trees. 

Step four is simple enough: just fit the two models: base and updated model. If there are multiple hypotheses to be tested, perhaps multiple updated models should be estimated, carefully considering when to add more than one variable at a time. 

Step five is where FOOSP shines as a staunch improvement over NHST. Simply take the cross-validated models and test their predictive accuracy on the left out test set. There are a variety of methods this can take, the simplest of which was described above: leaving aside about 1/3rd of the data for a test set. Other methods include leave-one-out cross-validation (LOOCV); during step three fit the model to the entire sample minus one, and test prediction on the left-out observation, repeat this process for all observations and average the mean-squared error across all iterations [@james2017]. This is obviously computationally intensive, but provides extensive cross-validation to learn and improve models from. In the cases of bootstrapping and bagging, it is normal to test on the training set, or 'out-of-bag.' 

Step six simply involves evaluating the information retreived from the previous five steps. In an ideal case one finds that adding the variables measured for the theory and hypothesis have substantively significantly improved the model's predictive accuracy. In this setting statistical significance is *never* part of the discussion because standard errors and residuals in the GoF sense are completely irrelevant to the central concerns of FOOSP.  Below, I elaborate on specific methods that can and should be used such as ridge regression, LASSO, k-nearest neighbors (KNN), Random Forests, and Support Vector Machines. Each of these models, as well as standard OLS and other generalized linear models (e.g. logistic regression) and much more also fit nicely into this general framework. Also below, I elaborate further on exactly why freedom from the tyranny of GoF, standard errors, and p-values makes sience easier. 

## The tyranny of residuals in NHST and FOOSP's liberation

Simply put, the foundation of NHST as discussed above is that models meet their strict assumptions. Under these met strict assumptions standard errors are correct and p-values can be calculated and null hypotheses can be rejected, but alternative hypotheses cannot be confirmed. NHST also does not allow the examination of predictive accuracy out-of-sample. NHST provides almost no value for scientific advancement of theories, despite science's best effort for decades to extract value. 

FOOSP, alternatively, does not have any concern, or even discussion of standard errors, or p-values, and thus does not have to meet those strict assumptions. Predictive models either predict well out of sample, or they do not. Theories, hypotheses, and variables either add predicted value, or they do not. Whereas under NHST not properly modeling temporal or spatial dynamics can result in a virtually uninterpretable model from an inferential perspective, predictive-based modeling is largely unaffected. If space and time are important, and unmodelled, then the model predicts poorly, but it still predicts. Moreover, the predictions that it makes, while predicting poorly, are still interpretable even if they model performs poorly. 

Thus, whereas under NHST researchers can fail to model important correlation in the residuals and fallaciously interpret the results when they contain little truth, FOOSP directly punishes for poorly modeling real dynamics by predicting poorly. In NHST, this factor is never even considered, and many journals and referees do not ask for spatio-temporal models on average. 

## Bias-variance trade-off

One theme with machine learning models and a general shift to FOOSP that must be discussed before moving forward is the bias-variance trade-off of predictive machine learning methods. Assume that there is some true model predicting $Y$, $Y = f(X)$. In trying to uncover this relationship, I estimate the relationship $\hat{Y} = \hat{f(x)} + \epsilon$ on a training set and intend to test this estimate relationship on a withheld test set as described above. Ideally, the best model at predicting $Y$ assuming $f(x)$ is linear will minimize mean squared error (MSE) $\frac{\sum_{i=1}^{n}{(y_i - \hat{y_i})^2}}{n}$ also written $\frac{\sum_{i=1}^{n}{(y_i - \hat{f(x_i)})^2}}{n}$ [@james2017, p. 29]. In the test set, the expected value of MSE is $E(y_0 - \hat{f(x_0)})^2 = Var(\hat{f(x_0)}) + Bias(\hat{f(x_0)})^2 + Var(\epsilon)$ where $y_0$ and $x_0$ denote observations from a previously unseen test set. $Var(\hat{f(x_0)})$ is the variance of the specified model if it were estimated using a different training set, and $Bias(\hat{f(x_0)})$ is the error "introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model [@james2017, pp. 34-35]", or how wrong it is in the test set.  

The bias-variance trade-off rewards parsimonious models with lower variance but punishes with higher bias on average, and it punishes complicated models with higher variance but lower bias. Intuitively, this mimics real world processes that political scientists try to model: a simple description of events likely does not predict when that event will occur well, but it will also not predict a large variety of conditions where it may occur. Alternatively, a complex description may predict many cases where the event may occur, and many of them will be correct. An ideal description is as simple as possible while predicting as correctly as possible, i.e. having low model variance and low model bias. Machine learning models, and out-of-sample predictive accuracy heuristics, are constantly combating this concept as a central tenet of what makes a model good or bad. The researchers must decide if they prefer variance of bias when both cannot be minimized. Some methods discused below are optimized to be lower in either bias or variance, but not always both. 

## Explanation of models 

### Penalized Linear Regression 

Penalized linear regression is one of the simplest and most easily executable alternatives to standard OLS regression. Penalized regression models optimize for prediction and for parsimony. OLS is incapable of producing a parsimonious model because all coefficients will be nonzero [@hindman2015, p. 56]. This can be particularly problematic in the context of modeling nonlinear or conditional relationships with linear model via quadratic/polynomial terms and interaction effects. Also, OLS is a poorer predictor in comparison to penalized regression models. this is because the model variance of OLS is relatively high, and thus dependent on the training data. A change in the training data can have a large impact on the prediction resulting from the model [@james2017, p. 218]. There are two common alternatives to OLS, ridge regression and least absolute shrinkage and selection operators (LASSO). Both are linear models that operate under similar principals. 

When one estimates OLS the resultant model minimizes $RSS = \sum_{i = 1}^n({y_i - \hat{y_i}})^2$. Ridge regression and LASSO regression operate in a similar framework that minimizes RSS in addition to a tuning (shrinkage) parameter and some linear combination of the resultant coefficients. For ridge regression the value being minimized is $RSS + \lambda\sum_{j = 1}^p\beta_j^2$ where *p* is the number of parameters in the model. LASSO minimizes $RSS + \lambda\sum_{j = 1}^p|\beta_j|$. In each value $\lambda$ is a tuning parameter set by the user to increase parsimony as its distance from zero increases. The tuning parameter is positive and ranges from zero to infinity. This tuning parameter should be selected via cross-validation methods and minimizing prediction error in a test set. Note that when the tuning parameter is set to zero the result is identical to OLS. 

The major difference between ridge regression and LASSO approaches is that LASSO will set coefficients on variables that are not very helpful in prediction to zero, whereas ridge regression is less parsimonious because its regularization method in the value to be minimized cannot set coefficients to zero, only very, very small. Both approaches have higher variance than OLS, because they will penalize the addition of variables that do not add much predicted value on the assumption that they are likely only predicting noise. 

Solving LASSO and ridge regression is notably more complex than OLS. One approach advocated for by @hindman2015 is least angle regression (LARS) algorithm, which iteratively increases the coefficient of the most-correlated variable from zero in the direction of the correlation until moving the coefficient away from zero does not improve model fit in a way that outways its added penalty. 

This approach, and these types of regression generally, do not rely on hypothesis testing and do not produce standard errors because their goals are not hypothesis testing. However, these models are very useful for identifying which variables are still important for predicting outcomes. First, if a variable's coefficient is nonzero in LASSO, or sufficiently far from zero that it is interesting in ridge regression, then a researcher can conlude that these variables are useful for predicting the outcome. In the context of testing new theoretical relationships, this can be helpful because an added variable is helpful in predicting empirically if and only if its coefficient is nonzero, or not close to zero. In other words, if a variable's added value in predicting the outcome (i.e. its reduction of RSS) is greater than its added penalty then it is theoretically relevant and useful. This approach can also be used to discern between a large set of variables to decide which are and are not relevant predictors. 

### K-nearest neighbors 

K-nearest neighbors (KNN) is a very simple classification method that thrives in identifying non-linear effects. It is quite similar to matching in some senses. Given a sample and a categorical outcome variable, k-nearest neighbors uses Bayes rule to classify an observations based on the K other most similar observations. Given K (the number of comparison observations to select) and a test observation $x_0$, KNN selects K closest points in a training set (this set of K points is denoted $N_0$). Then, it estimates the conditional probability that $x_0$ belongs category $j$ via $P(Y=j | X = x_0) = \frac{1}{k}\sum_{i\in N_0}I(y_i = j)$ [@james2017, p. 39]. 

This approach can adjudicate whether or not a variable and its related theory is useful for improving prediction by estimating a KNN model both with and without the new variable. If the model with the new variable improves prediction (either increases true positives/negatives or decreases false positives/negatives) then an argument can be made that this variable improves prediction and thus understanding of the outcome. Admittedly, this is a very 'black box' approach because it does not offer any output information on direction or effect size, but it is also not dependent on meeting virtually any assumptions unlike NHST. 

KNN is sensitive to the selection of K. As K increaes, the model becomes less flexible and varaince decreases thus typically increasing bias. The benefit of KNN is that it is nonparametric and nonlinear, thus allowing one to uncover a good predictive model without having to consider the shape of the relationship between those variables. Iferentially, though, this is a definite disadvantage. 


### Random Forests 

### Support Vector Machines

# NHST v. FOOSP in practice: an empirical example